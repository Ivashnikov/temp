{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2577dea4-7bb9-4e64-b352-877a8e767af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53577608-65b5-43f8-a6a8-b2dce441cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.expanduser('~/develop/ClearML_ML_SD.yml'), 'r') as f:\n",
    "    keys = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b81823-57dd-4b18-8cb9-6acfec748a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLEARML_WEB_HOST\"] = \"https://app.clear.ml\"\n",
    "os.environ[\"CLEARML_API_HOST\"] = \"https://api.clear.ml\"\n",
    "os.environ[\"CLEARML_FILES_HOST\"] = \"https://files.clear.ml\"\n",
    "os.environ[\"CLEARML_API_ACCESS_KEY\"] = keys['access_key']\n",
    "os.environ[\"CLEARML_API_SECRET_KEY\"] = keys['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f9fb85-dcf0-41f6-a137-85f81427ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4920af0c-79bb-4a46-914d-c99fe9377c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=d183beaa3d574de5b5cba1e883092136\n",
      "2022-11-08 22:42:38,873 - clearml.Repository Detection - WARNING - Failed accessing the jupyter server(s): []\n",
      "2022-11-08 22:42:38,886 - clearml.Task - INFO - No repository found, storing script code instead\n",
      "ClearML results page: https://app.clear.ml/projects/922c69dbd48249b183708fef50f18e10/experiments/d183beaa3d574de5b5cba1e883092136/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name='ML_SD', \n",
    "    task_name='cnn', \n",
    "    tags=['cnn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "498e7692-a703-428f-9949-9927d6c62d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# from torchtext.legacy import datasets\n",
    "# from torchtext.legacy.data import Field, LabelField\n",
    "# from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab, Vectors, GloVe\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7e65d3-0254-43f4-b352-59da890cd187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7714fa6b-ded8-4788-85e8-e627c03ccfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 21\n",
    "PATH = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8935fac6-bdc0-4d34-8f64-4cb7ac0cfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "df_val = pd.read_csv(os.path.join(PATH, 'val.csv'))\n",
    "df_test = pd.read_csv(os.path.join(PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34366831-a73c-4d14-982e-5db866ec2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_form(list_words, morph):\n",
    "    return [morph.parse(word)[0].normal_form for word in list_words]\n",
    "\n",
    "def del_stopwords(list_words, stop_words):\n",
    "    return [word for word in list_words if word not in stop_words]\n",
    "\n",
    "def transform_data(df):\n",
    "    df = df.copy()\n",
    "    df['level_2'] = df['icd10'].str.split('.').apply(lambda x: x[0])\n",
    "    df['level_1'] = df['icd10'].apply(lambda x: x[0])\n",
    "    df['symptoms_tokens'] = df['symptoms'] \\\n",
    "        .str.lower() \\\n",
    "        .str.split('[^a-zа-яё]+')\n",
    "#         .progress_apply(partial(del_stopwords, stop_words=get_stop_words('russian'))) \\\n",
    "#         .progress_apply(partial(norm_form, morph=MorphAnalyzer()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "acd1be6a-3ed2-4f70-8bd2-7af7ec3bdf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = transform_data(df_train)\n",
    "df_val = transform_data(df_val)\n",
    "df_test = transform_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65cd8b1f-cc1c-46ba-90b9-55a798ac120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y_train = df_train['level_2'].values\n",
    "# y_val = df_val['level_2'].values\n",
    "# y_test = df_test['level_2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268670ff-ac37-49a6-bfeb-9eb018355b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51360877-4ca3-4405-a032-10a5143eab28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ab73433b-e1e2-443d-a9fd-9887351ff143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab\n",
    "counter = Counter(np.concatenate(df_train['symptoms_tokens'].tolist()))\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab_text = vocab(ordered_dict, min_freq=10, specials=('<unk>', '<PAD>', '<BOS>', '<EOS>'))\n",
    "vocab_text.set_default_index(vocab_text['<unk>'])\n",
    "\n",
    "text_stoi = vocab_text.get_stoi()\n",
    "text_itos = {v: k for k, v in text_stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8db7a443-8794-45be-ad24-39c49c512fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, text_vocab: vocab):\n",
    "        self.texts = [[text_vocab[token] for token in text] for text in texts]\n",
    "        default_label = y_train.shape[1]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'X': self.texts[idx], 'y': self.labels[idx]} \n",
    "    \n",
    "\n",
    "class Collator(object):\n",
    "    def __init__(self, padding_value: int = 0, device: str = 'cpu', sort_key=None, batch_first=False):\n",
    "        self.padding_value = padding_value\n",
    "        self.sort_key = sort_key\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        if self.sort_key is not None:\n",
    "            batch = sorted(batch, key=self.sort_key)\n",
    "        \n",
    "        text = []\n",
    "        label = []\n",
    "        for item in batch:\n",
    "            text.append(torch.tensor(item['X']))\n",
    "            label.append(item['y'])\n",
    "        \n",
    "        text = pad_sequence(text, padding_value=self.padding_value)\n",
    "        if self.batch_first:\n",
    "            text = text.T\n",
    "            \n",
    "        label = torch.tensor(label)\n",
    "            \n",
    "        batch = {\n",
    "            'X': text, \n",
    "            'y': label,\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class BucketSampler:\n",
    "    def __init__(self, dataset, batch_size: int, drop_last: bool = False, sort_key=None, shuffle: bool = False) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.sort_key = sort_key\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.sort_key is not None:\n",
    "            indices = [(i, self.sort_key(item)) for i, item in enumerate(self.dataset)]\n",
    "            if self.shuffle:\n",
    "                random.shuffle(indices)\n",
    "            pooled_indices = []\n",
    "            # create pool of indices with similar lengths \n",
    "            for i in range(0, len(indices), self.batch_size * 100):\n",
    "                pooled_indices.extend(sorted(\n",
    "                    indices[i:i + self.batch_size * 100], \n",
    "                    key=lambda x: x[1], \n",
    "                ))\n",
    "            indices = [x[0] for x in pooled_indices]    \n",
    "        else:\n",
    "            indices = np.arange(len(self.dataset))\n",
    "            if self.shuffle:\n",
    "                random.shuffle(indices)\n",
    "\n",
    "        # yield indices for current batch\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            yield indices[i:i + self.batch_size]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.drop_last:\n",
    "            return len(self.dataset) // self.batch_size  \n",
    "        else:\n",
    "            return (len(self.dataset) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5804baf-bf58-41cf-b514-636b57b2126f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f842409-14da-438b-acae-26a3f32d97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_train.groupby('level_2', sort=False).transform('size') > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0655b691-beb1-4c15-a3e8-90b86a493e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "target_enc.fit(df_train.loc[mask, ['level_2']])\n",
    "\n",
    "X_train = df_train.loc[mask, 'symptoms_tokens'].tolist()\n",
    "X_val = df_val['symptoms_tokens'].tolist()\n",
    "X_test = df_test['symptoms_tokens'].tolist()\n",
    "\n",
    "y_train = target_enc.transform(df_train.loc[mask, ['level_2']])\n",
    "y_val = target_enc.transform(df_val[['level_2']])\n",
    "y_test = target_enc.transform(df_test[['level_2']])\n",
    "\n",
    "# create datasets\n",
    "train_dataset = TextDataset(X_train, y_train, vocab_text)\n",
    "val_dataset = TextDataset(X_val, y_val, vocab_text)\n",
    "test_dataset = TextDataset(X_test, y_test, vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "08c98ebd-6303-4c6a-af11-c91d414696fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011, 108)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8f41aea-67d3-4ab2-91d9-9c964a59d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = iter(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e1c6fbb-8293-496e-942b-d0ee070414e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': [16,\n",
       "  5,\n",
       "  354,\n",
       "  96,\n",
       "  9,\n",
       "  8,\n",
       "  41,\n",
       "  0,\n",
       "  56,\n",
       "  589,\n",
       "  407,\n",
       "  842,\n",
       "  635,\n",
       "  230,\n",
       "  12,\n",
       "  8,\n",
       "  185,\n",
       "  345,\n",
       "  374,\n",
       "  723,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1401,\n",
       "  12,\n",
       "  8,\n",
       "  185,\n",
       "  210,\n",
       "  345,\n",
       "  374,\n",
       "  28,\n",
       "  513,\n",
       "  159,\n",
       "  557,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1404,\n",
       "  0,\n",
       "  120,\n",
       "  709,\n",
       "  579,\n",
       "  88,\n",
       "  252,\n",
       "  6],\n",
       " 'y': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73d2b3-6f90-4ab2-ab8e-e1ccb14161f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360fad8-88f4-4656-b0c1-f8f9643c763d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "755abfce-079d-454c-803b-7f864ba11003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кастомные функции для обучения моделей\n",
    "import gc\n",
    "\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, loss_func, opt, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_func = loss_func\n",
    "        self.opt = opt\n",
    "        self.device = device\n",
    "            \n",
    "    def train_epoch(self, train_iter, epoch):\n",
    "        loss_value = 0.0\n",
    "        \n",
    "        y_fact = []\n",
    "        y_pred = []\n",
    "        self.model.train()\n",
    "        pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        for it, batch in pbar: \n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "            X = batch['X'].to(self.device)\n",
    "            y = batch['y'].to(self.device)\n",
    "            outputs = self.model(X)\n",
    "\n",
    "            loss = self.loss_func(outputs, y)\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            self.opt.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "\n",
    "            y_fact.append(batch['y'].numpy().argmax(axis=1))\n",
    "            y_pred.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "            pbar.set_description(f\"\"\"\n",
    "                Train Loss: {loss:.4}\n",
    "            \"\"\")\n",
    "\n",
    "        y_fact = np.hstack(y_fact)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        \n",
    "        metrics = dict(\n",
    "            loss = loss_value / len(train_iter),\n",
    "            hit3 = hit_at_n(y_fact, y_pred, n=3),\n",
    "            precision = hit_at_n(y_fact, y_pred, n=1),\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def eval_epoch(self, val_iter, epoch):\n",
    "        loss_value = 0.0\n",
    "        \n",
    "        y_fact = []\n",
    "        y_pred = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        pbar = tqdm(enumerate(val_iter), total=len(val_iter), leave=False)\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        with torch.no_grad():\n",
    "            for it, batch in pbar:\n",
    "                X = batch['X'].to(self.device)\n",
    "                y = batch['y'].to(self.device)\n",
    "                outputs = self.model(X)\n",
    "                loss = self.loss_func(outputs, y)\n",
    "                loss_value += loss.item()\n",
    "\n",
    "                y_fact.append(batch['y'].numpy().argmax(axis=1))\n",
    "                y_pred.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "                pbar.set_description(f\"\"\"\n",
    "                    Test Loss: {loss:.4}\n",
    "                \"\"\")\n",
    "\n",
    "        y_fact = np.hstack(y_fact)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        \n",
    "        metrics = dict(\n",
    "            loss = loss_value / len(val_iter),\n",
    "            hit3 = hit_at_n(y_fact, y_pred, n=3),\n",
    "            precision = hit_at_n(y_fact, y_pred, n=1),\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def train_loop(self, train_iter, valid_iter, max_epochs, patience):\n",
    "\n",
    "        min_loss = np.inf\n",
    "\n",
    "        cur_patience = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_metrics = self.train_epoch(train_iter, epoch)\n",
    "            clear_cache()\n",
    "            \n",
    "            val_metrics = self.eval_epoch(valid_iter, epoch)\n",
    "            clear_cache()\n",
    "            \n",
    "            val_loss = val_metrics['loss']\n",
    "            if val_loss < min_loss:\n",
    "                min_loss = val_loss\n",
    "                best_model = self.model.state_dict()\n",
    "            else:\n",
    "                cur_patience += 1\n",
    "                if cur_patience == patience:\n",
    "                    cur_patience = 0\n",
    "                    break\n",
    "            clear_output()\n",
    "            print('%20s: %2d' % ('epoch', epoch))\n",
    "            print()\n",
    "            print('%20s: %7.4f %3.4f' % ('loss', train_metrics['loss'], val_metrics['loss']))\n",
    "            print()\n",
    "            print('%20s: %7.4f %3.4f' % ('hit3', train_metrics['hit3'], val_metrics['hit3']))\n",
    "            print('%20s: %7.4f %3.4f' % ('precision', train_metrics['precision'], val_metrics['precision']))\n",
    "\n",
    "#             print(*[f'{k}: {v}' for k, v in train_metrics.items()])\n",
    "#             print(*[f'{k}: {v}' for k, v in val_metrics.items()])\n",
    "\n",
    "        self.model.load_state_dict(best_model)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9678df8e-6369-4e3a-aa5c-c6bfab87a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13704/1522143327.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  label = torch.tensor(label)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[ 16,   5, 354,  ...,   1,   1,   1],\n",
       "         [ 23,   0,   0,  ...,   1,   1,   1],\n",
       "         [  5,   7,   4,  ...,   1,   1,   1],\n",
       "         ...,\n",
       "         [189, 277,   7,  ...,   1,   1,   1],\n",
       "         [  5, 484,   7,  ...,   1,   1,   1],\n",
       "         [  7,   4, 232,  ...,   1,   1,   1]]),\n",
       " 'y': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8783bb-94fb-4f01-8b15-518fc36be374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed3293-f804-4a54-83e5-b9ffc787fb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053cad3a-bf9a-4ac9-ad8f-2a3061ed5c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86b93c-e161-4667-bddd-b14e4590e46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a886ad5-adff-4758-95d5-c94063ec72ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcf0b380-edc8-4a2d-bcd6-7c657cf9cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        emb_dim,\n",
    "        out_channels,\n",
    "        kernel_sizes,\n",
    "        dropout=0.5,\n",
    "        n_classes=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # num_filters = 36\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "        \n",
    "#         self.conv_0 = nn.Conv1d(emb_dim, out_channels, kernel_size=kernel_sizes[0], padding=1, stride=2)  # YOUR CODE GOES HERE     \n",
    "#         self.conv_1 = nn.Conv1d(emb_dim, out_channels, kernel_size=kernel_sizes[1], padding=1, stride=2)  # YOUR CODE GOES HERE\n",
    "#         self.conv_2 = nn.Conv1d(emb_dim, out_channels, kernel_size=kernel_sizes[2], padding=1, stride=2)  # YOUR CODE GOES HERE\n",
    "        \n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, out_channels, (K, emb_dim)) for K in kernel_sizes])\n",
    "        \n",
    "        # self.fc = nn.Linear(len(kernel_sizes) * out_channels, n_classes)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*out_channels, n_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        x = self.embedding(text)\n",
    "        \n",
    "        # embedded = embedded.permute(0, 2, 1)  # may be reshape here\n",
    "        \n",
    "#         conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here\n",
    "#         conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here\n",
    "#         conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here\n",
    "        \n",
    "#         pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "#         pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "#         pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "#         cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        \n",
    "            \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "15649363-4586-4f62-aa16-47a44528454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_Text(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(CNN_Text, self).__init__()\n",
    "#         filter_sizes = [1,2,3,5]\n",
    "#         num_filters = 36\n",
    "#         n_classes = len(le.classes_)\n",
    "#         self.embedding = nn.Embedding(max_features, embed_size)\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "#         self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)  \n",
    "#         x = x.unsqueeze(1)  \n",
    "#         x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "#         x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "#         x = torch.cat(x, 1)\n",
    "#         x = self.dropout(x)  \n",
    "#         logit = self.fc1(x) \n",
    "#         return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93dac1a1-3d7f-412d-b5f6-2fb118200b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_n(y_true, y_pred, n=3):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    score = np.mean(np.any(\n",
    "        np.argsort(-y_pred, axis=1)[:, :n] == y_true.reshape(-1,1), \n",
    "        axis=1\n",
    "    ))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7831dcb2-57d2-448f-940a-9e13c4270867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f87cb03-44d6-401c-aab1-266e53648de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9899b77c-1261-4a68-827b-8e32a777f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN(\n",
    "#     vocab_size=len(vocab_text), \n",
    "#     emb_dim=300, # 300, \n",
    "#     out_channels=512, # 64,\n",
    "#     kernel_sizes=[1, 2, 3, 4, 5], # [3, 4, 5], \n",
    "#     dropout=0.8, # 0.5\n",
    "#     n_classes=y_train.shape[1],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model, \n",
    "#     loss_func=nn.CrossEntropyLoss(reduction='sum'), \n",
    "#     opt=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "#     device='cuda'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56ab397e-b28f-44a5-bd91-209ddef497fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders = {\n",
    "#     name: DataLoader(\n",
    "#         dataset, \n",
    "#         batch_sampler=BucketSampler(dataset, batch_size=512, shuffle=name=='train'),\n",
    "#         collate_fn=Collator(padding_value=vocab_text['<PAD>'], batch_first=True),\n",
    "#     )\n",
    "#     for name, dataset in zip(['train', 'val', 'test'], [train_dataset, val_dataset, test_dataset])\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# trainer.train_loop(loaders['train'], loaders['val'], max_epochs=20, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "db871995-2ac7-489c-b85e-362aef9431e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6ccca1f-b019-45bb-91dd-f09bdfeb4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f240ca40-b072-4767-9964-8293f58df2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = KeyedVectors.load_word2vec_format('~/Downloads/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70410ca2-ef3e-4f9c-8ab1-f0b01a2ecefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('боли', 0.8303021788597107),\n",
       " ('тошноту', 0.731131374835968),\n",
       " ('тошнота', 0.7193885445594788),\n",
       " ('болезненность', 0.7105488181114197),\n",
       " ('головокружение', 0.6976202726364136),\n",
       " ('сонливость', 0.6919495463371277),\n",
       " ('жалость', 0.6907328963279724),\n",
       " ('болью', 0.684307336807251),\n",
       " ('рвоту', 0.6833842396736145),\n",
       " ('раздражительность', 0.6796847581863403)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similar_by_word('боль')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "790e2800-aaf2-4084-a1d9-3f36037a498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = torch.Tensor(np.vstack([\n",
    "    vectors[word] if word in vectors else np.zeros(300) for ind, word \n",
    "    in sorted(text_itos.items(), key=lambda x: x[0], reverse=False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a39872-ab28-4382-93b1-a2b31e71b714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5a2d5325-9acc-49fc-81a7-7d9ce5109a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    vocab_size=len(vocab_text), \n",
    "    emb_dim=300, # 300, \n",
    "    out_channels=512, # 64,\n",
    "    kernel_sizes=[3, 4, 5], # [3, 4, 5], \n",
    "    dropout=0.5, # 0.5\n",
    "    n_classes=y_train.shape[1],\n",
    ")\n",
    "\n",
    "# prev_shape = model.embedding.weight.shape\n",
    "# model.embedding.weight = nn.Parameter(word_embeddings)\n",
    "# model.embedding.weight.requires_grad = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    loss_func=nn.CrossEntropyLoss(reduction='sum'), \n",
    "    opt=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e828ca96-c5c3-4c83-a67e-1327e1565c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.embedding.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "088abca8-8bb6-4cea-b2d4-8b89068597d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampler(dataset, log_counts=False):\n",
    "    \"\"\"\n",
    "    функция возвращает семплер, для балансировки класса\n",
    "    \"\"\"\n",
    "    target = np.argmax(train_dataset.labels, axis=1)\n",
    "    class_sample_counts = np.unique(target, return_counts=True)[1]\n",
    "    if log_counts:\n",
    "        class_sample_counts = np.log(class_sample_counts)\n",
    "    weight = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "    samples_weight = weight[target]\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ce014f4c-0b32-47b4-99eb-520b2bb888c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               epoch: 26\n",
      "\n",
      "                loss:  2.7951 482.6909\n",
      "\n",
      "                hit3:  1.0000 0.5752\n",
      "           precision:  0.9996 0.3851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaders = {\n",
    "    name: DataLoader(\n",
    "        dataset, \n",
    "        # batch_sampler=BucketSampler(dataset, batch_size=512, shuffle=name=='train'),\n",
    "        collate_fn=Collator(padding_value=vocab_text['<PAD>'], batch_first=True),\n",
    "        sampler=get_sampler(dataset, log_counts=True) if name=='train' else None,\n",
    "        batch_size=256, \n",
    "    )\n",
    "    for name, dataset in zip(['train', 'val', 'test'], [train_dataset, val_dataset, test_dataset])\n",
    "}\n",
    "\n",
    "\n",
    "trainer.train_loop(loaders['train'], loaders['val'], max_epochs=40, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b58c0f41-25fa-4b7c-8f7c-c2dafa74431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = next(iter(loaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "869d4c01-4eae-4960-8d7b-71350cae16a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 435])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bb68e4fc-9cad-45e1-81a0-e269e48856ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 108])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9fc43-8da4-4f94-ac14-0bd078b43e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
